#### 感知机
##### 简单感知机
	* 组成结构  
	* 简单实现 
		1. 与门，非门，或门  
		![与门实现代码]()
		2. 加入权重和偏置后表达式  
		![感知机加入权重和偏置表达式]()  
		3. 加入权重和偏置后实现
		![感知机加入权重和偏置实现]()  
		
	* 局限性  
		1. 简单感知机具有局限性，具体表现在用直线分类，而对有些问题无法使用直线简单分类，如下：  
		![简单感知机的局限]()  

	* 总结
		1. 简单感知机是深度学习起源的算法，能够解决一些简单的问题  
		2. 简单感知机通过输入+权重+偏置+激活函数(阶跃函数)+输出组成  
		3. 权重控制输入信号的重要性，偏置调整激活的难易程度  
		4. 简单感知机具有局限性，无法划分复杂的问题(异或门)  

##### 多层感知机
	* 理解
		1. 在感知机的基础上将与、或、非运算结合，可以实现异或运算，从而突破了简单感知机的局限性  
		2. 多层感知机的应用 -> 计算机 -> 神经网络  
		3. 多层感知机即为神经网络  
#### 神经网络  
##### 神经网络组成  
	* 组成结构  
		1. 基本组成 ： 输入层 + 中间层 + 输出层  
		2. 输入层 ： 输入信号  
		3. 输入层 -> 中间层, 中间层内部 ： 权重系数， 偏置函数 ， 激活函数(sigmoid 函数，ReLU函数)  
		4. 中间层 -> 输出层 ： 权重系数， 偏置函数， softmax函数(分类)/恒等函数(回归)   
 
	* 激活函数  
		1. sigmoid函数
			1. 表达式  
			![sigmoid函数表达式]()  
			2. 实现  
			![sigmoid函数实现]()  
			3. sigmoid函数图  
			![sigmoid函数图]()  

		2. ReLU函数  
			1. 表达式  
			![ReLU表达式]()  
			2. 实现  
			![ReLU函数实现]()
			3. ReLU示意图  
			![ReLU示意图]() 

	* softmax函数  
		1. 表达式  
			![Softmax函数表达式]()  
		2. 实现  
			![Softmax函数实现]()  
		3. Softmax示意图  
			![Softmax示意图]()
		4. Softmax函数存在的缺陷 ： 发生数值溢出  
		5. Softmax函数改进后表达式  
		 	![Soft函数改进后表达式]()
		6. Softmax函数改进后实现 ： 
			![Soft函数改进后实现]
		7. Softmax函数特性 ： 函数输出的值的总和是1，由此可以将不同的输出的值称为概率。  
	
	* 对神经网路组成的总结  
		1. 神经网络中的激活函数使用平滑变化的sigmoid函数或ReLU函数。  
		2. 通过巧妙地使用NumPy多维数组，可以高效地实现神经网络。  
		3. 机器学习的问题大体上可以分为回归问题和分类问题。  
		4. 关于输出层的激活函数，回归问题中一般用恒等函数，分类问题中一般用 softmax 函数。  
		5. 分类问题中，输出层的神经元的数量设置为要分类的类别数。  
		6. 输入数据的集合称为批。通过以批为单位进行推理处理，能够实现高速的运算。  

##### 神经网络的学习 
	* 神经网络学习过程  
		1. 构建神经网络组成：输入层，隐藏层，输出层 
		2. 确定各个层级之间的权重，偏置，激活函数，输出函数  
		3. 利用数据集中的数据，通过学习模型得到输出值  
		4. 确定使用的损失函数，求得损失值  
		5. 利用梯度下降法或反向误差传播算法，求得损失值最小的w和b  
		6. 确定学习模型，通过测试数据测试  

	* 神经网络通过mini-batch确定训练数据集  
		1. 在数据量较大的时候，减少计算的数据量  
 
	* 神经网络通过数据驱动-> 在数据中学习
		1. 训练数据：寻找最优的参数  
		2. 测试数据：评价训练得到的模型的世纪能力  
		3. 追求泛化能力： 解决欠拟合和过拟合的问题    
		  
	* 神经网络通过损失函数寻找最优参数-> 最优模型  
		1. 损失函数-均方误差  
			1. 表达式  
			![均方差函数表达式]()  
			2. 实现  
			![均方差函数实现]()  
			3. 均方差函数图  
			![均方差函数图]()  

		2. 损失函数-交叉熵误差  
			1. 表达式  
			![交叉熵误差函数表达式]()  
			2. 实现  
			![交叉熵误差函数实现]()  
			3. 均方差函数图  
			![交叉熵误差函数图]()  

	* 神经网络训练使用数据集的方法-> mini-batch学习  
		1. 使用最小数据集的原因：需要对所有输入的损失函数值求和,计算量过大  
	
	*
